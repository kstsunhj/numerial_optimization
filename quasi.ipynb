{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Self-contained implementation of non-linear optimization algorithms:\n",
    "- steepest descent\n",
    "- newton's method\n",
    "- conjuage gradient\n",
    "- BFGS\n",
    "- l-BFGS\n",
    "Following Nocedal & Wright's Numerical Optimization Chapter 3, 5 & 8\n",
    "Yiren Lu, Jun 2017\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 2d rosenbrock function and its first and second order derivatives\n",
    "#     https://en.wikipedia.org/wiki/Rosenbrock_function\n",
    "def rosenbrock(x):\n",
    "    return 100 * (x[1] - x[0]**2)**2 + (1 - x[0])**2\n",
    "\n",
    "\n",
    "def grad_rosen(x):\n",
    "    return np.array([200*(x[1]-x[0]**2)*(-2*x[0]) + 2*(x[0]-1), 200*(x[1]-x[0]**2)])\n",
    "\n",
    "\n",
    "def hessian_rosen(x):\n",
    "    return np.array([[1200*x[0]**2 - 400*x[1] + 2, -400*x[0]], [-400*x[0], 200]])\n",
    "\n",
    "\n",
    "# line-search conditions\n",
    "def wolfe(f, g, xk, alpha, pk):\n",
    "    c1 = 1e-4\n",
    "    return f(xk + alpha * pk) <= f(xk) + c1 * alpha * np.dot(g(xk), pk)\n",
    "\n",
    "\n",
    "def strong_wolfe(f, g, xk, alpha, pk, c2):\n",
    "    # typically, c2 = 0.9 when using Newton or quasi-Newton's method.\n",
    "    #            c2 = 0.1 when using non-linear conjugate gradient method.\n",
    "    return wolfe(f, g, xk, alpha, pk) and abs(\n",
    "        np.dot(g(xk + alpha * pk), pk)) <= c2 * abs(np.dot(g(xk), pk))\n",
    "\n",
    "\n",
    "def gold_stein(f, g, xk, alpha, pk, c):\n",
    "    return (f(xk) + (1 - c) * alpha * np.dot(g(xk), pk) <= f(xk + alpha * pk)\n",
    "            ) and (f(xk + alpha * pk) <= f(xk) + c * alpha * np.dot(g(xk), pk))\n",
    "\n",
    "\n",
    "# line-search step len\n",
    "def step_length(f, g, xk, alpha, pk, c2):\n",
    "    return interpolation(f, g,\n",
    "                       lambda alpha: f(xk + alpha * pk),\n",
    "                       lambda alpha: np.dot(g(xk + alpha * pk), pk),\n",
    "                        alpha, c2,\n",
    "                        lambda f, g, alpha, c2: strong_wolfe(f, g, xk, alpha, pk, c2))\n",
    "\n",
    "\n",
    "def interpolation(f, g, f_alpha, g_alpha, alpha, c2, strong_wolfe_alpha, iters=20):\n",
    "    # referred implementation here:\n",
    "    # https://github.com/tamland/non-linear-optimization\n",
    "    l = 0.0\n",
    "    h = 1.0\n",
    "    for i in xrange(iters):\n",
    "        if strong_wolfe_alpha(f, g, alpha, c2):\n",
    "            return alpha\n",
    "\n",
    "    half = (l + h) / 2\n",
    "    alpha = - g_alpha(l) * (h**2) / (2 * (f_alpha(h) - f_alpha(l) - g_alpha(l) * h))\n",
    "    if alpha < l or alpha > h:\n",
    "        alpha = half\n",
    "    if g_alpha(alpha) > 0:\n",
    "        h = alpha\n",
    "    elif g_alpha(alpha) <= 0:\n",
    "        l = alpha\n",
    "    return alpha\n",
    "\n",
    "\n",
    "# optimization algorithms\n",
    "def steepest_descent(f, grad, x0, iterations, error):\n",
    "    x = x0\n",
    "    x_old = x\n",
    "    c2 = 0.9\n",
    "    for i in xrange(iterations):\n",
    "        pk = -grad(x)\n",
    "        alpha = step_length(f, grad, x, 1.0, pk, c2)\n",
    "        x = x + alpha * pk\n",
    "        if i % 10 == 0:\n",
    "            # print \"  iter={}, grad={}, alpha={}, x={}, f(x)={}\".format(i, pk, alpha, x, f(x))\n",
    "            print(\"  iter={}, x={}, f(x)={}\").format(i, x, f(x))\n",
    "\n",
    "        if np.linalg.norm(x - x_old) < error:\n",
    "            break\n",
    "        x_old = x\n",
    "    return x, i\n",
    "\n",
    "\n",
    "def newton(f, g, H, x0, iterations, error):\n",
    "    x = x0\n",
    "    x_old = x\n",
    "    c2 = 0.9\n",
    "    for i in xrange(iterations):\n",
    "        pk = -np.linalg.solve(H(x), g(x))\n",
    "        alpha = step_length(f, g, x, 1.0, pk, c2)\n",
    "        x = x + alpha * pk\n",
    "        if i % 50 == 0:\n",
    "        # print \"  iter={}, grad={}, alpha={}, x={}, f(x)={}\".format(i, pk, alpha, x, f(x))\n",
    "            print(\"  iter={}, x={}, f(x)={}\").format(i, x, f(x))\n",
    "\n",
    "        if np.linalg.norm(x - x_old) < error:\n",
    "            break\n",
    "        x_old = x\n",
    "    return x, i + 1\n",
    "\n",
    "\n",
    "def conjugate_gradient(f, g, x0, iterations, error):\n",
    "    xk = x0\n",
    "    c2 = 0.1\n",
    "\n",
    "    fk = f(xk)\n",
    "    gk = g(xk)\n",
    "    pk = -gk\n",
    "\n",
    "    for i in xrange(iterations):\n",
    "        alpha = step_length(f, g, xk, 1.0, pk, c2)\n",
    "        xk1 = xk + alpha * pk\n",
    "        gk1 = g(xk1)\n",
    "        beta_k1 = np.dot(gk1, gk1) / np.dot(gk, gk)\n",
    "        pk1 = -gk1 + beta_k1 * pk\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            # print \"  iter={}, grad={}, alpha={}, x={}, f(x)={}\".format(i, pk, alpha, xk, f(xk))\n",
    "            print (\"  iter={}, x={}, f(x)={}\").format(i, xk, f(xk))\n",
    "\n",
    "        if np.linalg.norm(xk1 - xk) < error:\n",
    "            xk = xk1\n",
    "            break\n",
    "\n",
    "        xk = xk1\n",
    "        gk = gk1\n",
    "        pk = pk1\n",
    "\n",
    "    return xk, i + 1\n",
    "\n",
    "\n",
    "def bfgs(f, g, x0, iterations, error):\n",
    "    xk = x0\n",
    "    c2 = 0.9\n",
    "    I = np.identity(xk.size)\n",
    "    Hk = I\n",
    "\n",
    "    for i in xrange(iterations):\n",
    "        # compute search direction\n",
    "        gk = g(xk)\n",
    "        pk = -Hk.dot(gk)\n",
    "\n",
    "        # obtain step length by line search\n",
    "        alpha = step_length(f, g, xk, 1.0, pk, c2)\n",
    "\n",
    "        # update x\n",
    "        xk1 = xk + alpha * pk\n",
    "        gk1 = g(xk1)\n",
    "\n",
    "        # define sk and yk for convenience\n",
    "        sk = xk1 - xk\n",
    "        yk = gk1 - gk\n",
    "\n",
    "        # compute H_{k+1} by BFGS update\n",
    "        rho_k = float(1.0 / yk.dot(sk))\n",
    "\n",
    "        Hk1 = (I - rho_k * np.outer(sk, yk)).dot(Hk).dot(I - \\\n",
    "           rho_k * np.outer(yk, sk)) + rho_k * np.outer(sk, sk)\n",
    "\n",
    "        if i % 10 == 0:\n",
    "        # print \"  iter={}, grad={}, alpha={}, x={}, f(x)={}\".format(i, pk, alpha, xk, f(xk))\n",
    "            print(\"  iter={}, x={}, f(x)={}\").format(i, xk, f(xk))\n",
    "\n",
    "        if np.linalg.norm(xk1 - xk) < error:\n",
    "            xk = xk1\n",
    "            break\n",
    "\n",
    "        Hk = Hk1\n",
    "        xk = xk1\n",
    "\n",
    "    return xk, i + 1\n",
    "\n",
    "\n",
    "def l_bfgs(f, g, x0, iterations, error, m=10):\n",
    "    xk = x0\n",
    "    c2 = 0.9\n",
    "    I = np.identity(xk.size)\n",
    "    Hk = I\n",
    "\n",
    "    sks = []\n",
    "    yks = []\n",
    "\n",
    "    def Hp(H0, p):\n",
    "        m_t = len(sks)\n",
    "        q = g(xk)\n",
    "        a = np.zeros(m_t)\n",
    "        b = np.zeros(m_t)\n",
    "        for i in reversed(xrange(m_t)):\n",
    "            s = sks[i]\n",
    "            y = yks[i]\n",
    "            rho_i = float(1.0 / y.T.dot(s))\n",
    "            a[i] = rho_i * s.dot(q)\n",
    "            q = q - a[i] * y\n",
    "\n",
    "            r = H0.dot(q)\n",
    "\n",
    "        for i in xrange(m_t):\n",
    "            s = sks[i]\n",
    "            y = yks[i]\n",
    "            rho_i = float(1.0 / y.T.dot(s))\n",
    "            b[i] = rho_i * y.dot(r)\n",
    "            r = r + s * (a[i] - b[i])\n",
    "\n",
    "        return r\n",
    "\n",
    "    for i in xrange(iterations):\n",
    "        # compute search direction\n",
    "        gk = g(xk)\n",
    "        pk = -Hp(I, gk)\n",
    "\n",
    "        # obtain step length by line search\n",
    "        alpha = step_length(f, g, xk, 1.0, pk, c2)\n",
    "\n",
    "        # update x\n",
    "        xk1 = xk + alpha * pk\n",
    "        gk1 = g(xk1)\n",
    "\n",
    "        # define sk and yk for convenience\n",
    "        sk = xk1 - xk\n",
    "        yk = gk1 - gk\n",
    "\n",
    "        sks.append(sk)\n",
    "        yks.append(yk)\n",
    "        if len(sks) > m:\n",
    "            sks = sks[1:]\n",
    "            yks = yks[1:]\n",
    "\n",
    "        # compute H_{k+1} by BFGS update\n",
    "        rho_k = float(1.0 / yk.dot(sk))\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print (\"  iter={}, grad={}, alpha={}, x={}, f(x)={}\").format(i, pk, \\\n",
    "            alpha, xk, f(xk))\n",
    "\n",
    "        if np.linalg.norm(xk1 - xk) < error:\n",
    "            xk = xk1\n",
    "            break\n",
    "\n",
    "        xk = xk1\n",
    "\n",
    "    return xk, i + 1\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  x0 = np.array([0, 0])\n",
    "  error = 1e-4\n",
    "  max_iterations = 1000\n",
    "\n",
    "  print '\\n======= Steepest Descent ======\\n'\n",
    "  start = time.time()\n",
    "  x, n_iter = steepest_descent(rosenbrock, grad_rosen, x0,\n",
    "                               iterations=max_iterations, error=error)\n",
    "  end = time.time()\n",
    "  print \"  Steepest Descent terminated in {} iterations, x = {}, f(x) = {}, time elapsed {}, time per iter {}\"\\\n",
    "    .format(n_iter, x, rosenbrock(x), end - start, (end - start) / n_iter)\n",
    "\n",
    "  print '\\n======= Conjugate Gradient Method ======\\n'\n",
    "  start = time.time()\n",
    "  x, n_iter = conjugate_gradient(rosenbrock, grad_rosen, x0,\n",
    "                                 iterations=max_iterations, error=error)\n",
    "  end = time.time()\n",
    "  print \"  Conjugate Gradient Method terminated in {} iterations, x = {}, f(x) = {}, time elapsed {}, time per iter {}\"\\\n",
    "    .format(n_iter, x, rosenbrock(x), end - start, (end - start) / n_iter)\n",
    "\n",
    "  print '\\n======= Newton\\'s Method ======\\n'\n",
    "  start = time.time()\n",
    "  x, n_iter = newton(rosenbrock, grad_rosen, hessian_rosen, x0,\n",
    "                     iterations=max_iterations, error=error)\n",
    "  end = time.time()\n",
    "  print \"  Newton\\'s Method terminated in {} iterations, x = {}, f(x) = {}, time elapsed {}, time per iter {}\" \\\n",
    "    .format(n_iter, x, rosenbrock(x), end - start, (end - start) / n_iter)\n",
    "\n",
    "  print '\\n======= Broyden-Fletcher-Goldfarb-Shanno ======\\n'\n",
    "  start = time.time()\n",
    "  x, n_iter = bfgs(rosenbrock, grad_rosen, x0,\n",
    "                   iterations=max_iterations, error=error)\n",
    "  end = time.time()\n",
    "  print \"  BFGS terminated in {} iterations, x = {}, f(x) = {}, time elapsed {}, time per iter {}\"\\\n",
    "    .format(n_iter, x, rosenbrock(x), end - start, (end - start) / n_iter)\n",
    "\n",
    "  print '\\n======= Limited memory Broyden-Fletcher-Goldfarb-Shanno ======\\n'\n",
    "  start = time.time()\n",
    "  x, n_iter = l_bfgs(rosenbrock, grad_rosen, x0,\n",
    "                     iterations=max_iterations, error=error)\n",
    "  end = time.time()\n",
    "  print \"  l-BFGS terminated in {} iterations, x = {}, f(x) = {}, time elapsed {}, time per iter {}\"\\\n",
    "    .format(n_iter, x, rosenbrock(x), end - start, (end - start) / n_iter)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
