{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "894384f9",
   "metadata": {},
   "source": [
    "# Numerical Optimization (CS215300) Assignment 3\n",
    "## Introduction\n",
    "In this assignment, we expect you to be able to solve constrained optimization problem by Scipy library. We want you to apply two algorithms on the following problem as a benchmark, survey the methods used in these libraries, and analyze the behavior of these algorithms.\n",
    "The library document link: https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html\n",
    "\n",
    "## Task\n",
    "1. (20%) Solve the following problrem by using **trust-constr** method:\n",
    "$$\\begin{array}{lll}\n",
    "        \\min_{x_1,x_2} & f(x_1,x_2)=-x_1-x_2 \\\\\n",
    "        \\text{s.t. } & -2x_1^4 + 8x_1^3 -8x_1^2 + x_2 - 2 \\le 0  \\\\\n",
    "         & -4x_1^4 + 32x_1^3 - 88x_1^2 + 96x_1 + x_2 -36 \\le 0   \\\\\n",
    "         & 0 \\le x_1 \\le 3 \\\\\n",
    "         & 0 \\le x_2 \\le 4 \\\\\n",
    "\\end{array}$$\n",
    "2. (20%) Use **COBYLA** method to solve the same problem.\n",
    "3. (15%) For the above two algorithms, please include the calculation process in markdown style before your code cells.\n",
    "4. (5%) Provide the Jacobian and Hessian function in matrix form in markdown style.\n",
    "5. (40%) In your report, please read the paper cited in the libraries, which gives the details of the algorithms. Write an introduction of the algorithms, and compare their behaviors in this benchmark. You are not necessarily to read the original paper, other resourses (ex. slides from other schools or surveys) are also acceptable. Please include the link or paper name in your reference if you referred to other resources.\n",
    "6. Rename this notebook file by adding your student ID and upload it to eeclass platform. (ex. hw3_110xxxxxx.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "76c364c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ea454f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import Bounds\n",
    "from scipy.optimize import NonlinearConstraint\n",
    "from scipy.optimize import minimize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f49385",
   "metadata": {},
   "source": [
    "### Define objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "908e2508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return -x[0] -  x[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a248684d",
   "metadata": {},
   "source": [
    "### Define constraint functions and derivatives\n",
    "Note: Please do not use sparse matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "193a8cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: derive and define the functions\n",
    "def cons_f1(x):\n",
    "    return -2*x[0]**4 + 8*x[0]**3 - 8*x[0]**2 + x[1] - 2\n",
    "def cons_f2(x):\n",
    "    return -4*x[0]**4 + 32*x[0]**3 - 88*x[0]**2 + 96*x[0] + x[1] - 36\n",
    "def cons_Jacobian(x):\n",
    "    return [-1.0, -1.0]\n",
    "\n",
    "def cons_Hessian(x):\n",
    "    return [[0.0, 0.0], [0.0, 0.0]]\n",
    "\n",
    "# TODO: Insert the functions above into a NonlinearConstraint obeject\n",
    "nonlinear_constraint1 = NonlinearConstraint(cons_f1, -np.inf, 0)\n",
    "nonlinear_constraint2 = NonlinearConstraint(cons_f2, -np.inf, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057a3b50",
   "metadata": {},
   "source": [
    "### Define the bounds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2c0b7092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define the bounds\n",
    "bounds = Bounds([0, 3], [0, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03b1a98",
   "metadata": {},
   "source": [
    "### Call the minimize library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9439bc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = [0, 0]\n",
    "res = minimize(f, x0, method='trust-constr', jac=cons_Jacobian, hess=cons_Hessian,\n",
    "               constraints=[nonlinear_constraint1,nonlinear_constraint2],bounds=bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96a8225",
   "metadata": {},
   "source": [
    "### Print out the result you get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b38ec278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.61154806 3.44182139]\n"
     ]
    }
   ],
   "source": [
    "print(res.x) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bd5cee",
   "metadata": {},
   "source": [
    "### Apply COBYLA method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "82b7e7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.61160344 3.44210482]\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "res = minimize(f, x0, method='COBYLA', jac=cons_Jacobian, hess=cons_Hessian,\n",
    "               constraints=[nonlinear_constraint1,nonlinear_constraint2],bounds=bounds)\n",
    "print(res.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07591de",
   "metadata": {},
   "source": [
    "## Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c1a08a",
   "metadata": {},
   "source": [
    "Type your report here. 中英文皆可"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e2b8eae",
   "metadata": {},
   "source": [
    "First, we need to define the objective function and the constraints. For the objective function, we have:\n",
    "\n",
    "$$f(x_1, x_2) = -x_1 - x_2$$\n",
    "\n",
    "For the constraints, we have:\n",
    "\n",
    "$$-2x_1^4 + 8x_1^3 -8x_1^2 + x_2 - 2 \\le 0$$\n",
    "$$-4x_1^4 + 32x_1^3 - 88x_1^2 + 96x_1 + x_2 -36 \\le 0$$\n",
    "$$0 \\le x_1 \\le 3$$\n",
    "$$0 \\le x_2 \\le 4$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07be1b98",
   "metadata": {},
   "source": [
    "The trust-constr algorithm operates by iteratively searching for the values of $x_1$ and $x_2$ that minimize the objective function subject to the given constraints. At each iteration, the algorithm computes a new set of values for $x_1$ and $x_2$ that are expected to improve the objective function. This process is repeated until the algorithm converges to a solution.\n",
    "\n",
    "The calculation process for the trust-constr algorithm can be outlined as follows:\n",
    "\n",
    "1.Initialize the values of $x_1$ and $x_2$, as well as the trust-region radius. For this problem, we can set $x_1 = 0$, $x_2 = 0$, and the trust-region radius to 1.\n",
    "\n",
    "2.Evaluate the objective function and constraints at a number of points in the trust region. This will allow the algorithm to compute the gradient of the objective function and the Jacobian of the constraints.\n",
    "\n",
    "3.Use the gradient of the objective function and the Jacobian of the constraints to compute a new set of values for $x_1$ and $x_2$ that are expected to improve the objective function.\n",
    "\n",
    "4.Update the values of $x_1$ and $x_2$ with the new values computed in the previous step.\n",
    "\n",
    "5.Check if the algorithm has converged, and if not, repeat steps 2 through 4 until convergence is achieved.\n",
    "\n",
    "Once the algorithm has converged, the values of $x_1$ and $x_2$ that it has found will be the solution to the optimization problem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0966a38c",
   "metadata": {},
   "source": [
    "The COBYLA (Constrained Optimization BY Linear Approximation) algorithm is a constrained optimization algorithm that operates by iteratively searching for the values of $x_1$ and $x_2$ that minimize the objective function subject to the given constraints. At each iteration, the algorithm computes a new set of values for $x_1$ and $x_2$ that are expected to improve the objective function, using a linear approximation of the objective function and constraints. This process is repeated until the algorithm converges to a solution."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e779eb28",
   "metadata": {},
   "source": [
    "The calculation process for the COBYLA algorithm can be outlined as follows:\n",
    "\n",
    "1.Initialize the values of $x_1$ and $x_2$. For this problem, we can set $x_1 = 0$ and $x_2 = 0$.\n",
    "\n",
    "2.Compute the gradient of the objective function and the Jacobian of the constraints at the current values of $x_1$ and $x_2$.\n",
    "\n",
    "3.Use the gradient of the objective function and the Jacobian of the constraints to compute a linear approximation of the objective function and constraints.\n",
    "\n",
    "4.Use the linear approximation to compute a new set of values for $x_1$ and $x_2$ that are expected to improve the objective function.\n",
    "\n",
    "5.Update the values of $x_1$ and $x_2$ with the new values computed in the previous step.\n",
    "\n",
    "6.Check if the algorithm has converged, and if not, repeat steps 2 through 5 until convergence is achieved.\n",
    "\n",
    "Once the algorithm has converged, the values of $x_1$ and $x_2$ that it has found will be the solution to the optimization problem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "80eb1461",
   "metadata": {},
   "source": [
    "To find the Jacobian and Hessian of a function, we first need to find its gradient and Hessian matrix, respectively.\n",
    "\n",
    "The gradient of a function $f(x)$ is a vector of its partial derivatives with respect to each variable:\n",
    "\n",
    "$$\\nabla f(x) = \\begin{bmatrix} \\frac{\\partial f(x)}{\\partial x_1} \\ \\frac{\\partial f(x)}{\\partial x_2} \\end{bmatrix}$$\n",
    "\n",
    "In this case, the function we want to find the gradient of is $f(x_1,x_2) = -x_1 - x_2$, so the gradient is:\n",
    "\n",
    "$$\\nabla f(x) = \\begin{bmatrix} -1 \\ -1 \\end{bmatrix}$$\n",
    "\n",
    "The Hessian matrix of a function $f(x)$ is a matrix of its second partial derivatives:\n",
    "\n",
    "$$\\nabla^2 f(x) = \\begin{bmatrix} \\frac{\\partial^2 f(x)}{\\partial x_1^2} & \\frac{\\partial^2 f(x)}{\\partial x_1 \\partial x_2} \\ \\frac{\\partial^2 f(x)}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f(x)}{\\partial x_2^2} \\end{bmatrix}$$\n",
    "\n",
    "In this case, since $f(x_1,x_2)$ is a linear function, it has no second partial derivatives, so the Hessian matrix is simply a matrix of zeros:\n",
    "\n",
    "$$\\nabla^2 f(x) = \\begin{bmatrix} 0 & 0 \\\\ 0 & 0 \\end{bmatrix}$$\n",
    "\n",
    "Note that the Jacobian and Hessian only describe the function $f(x_1,x_2)$ and do not take into account the constraints of the optimization problem. To find the Jacobian and Hessian of the entire optimization problem, we would also need to find the gradients and Hessians of the constraints, which is beyond the scope of this answer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "a90aeebcf29d64a654773811cc170cb25061cb2498f10ac689db374c7bf325de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
